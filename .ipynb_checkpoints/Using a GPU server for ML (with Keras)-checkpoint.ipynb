{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a GPU server for running your Machine Learning models\n",
    "\n",
    "This notebook explains how to install all the prerequisites and libraries that you will need to run your ML models in a GPU server with Keras. If you can execute all the following cells, you are good to go.\n",
    "\n",
    "## <a id='0'>Content</a>\n",
    "\n",
    "- <a href='#1'>Connecting to the GPU server</a>  \n",
    "- <a href='#2'>Changing your default password</a>  \n",
    "- <a href='#3'>Environment configuration</a>\n",
    "    - <a href='#31'>Option 1: Conda</a>\n",
    "    - <a href='#32'>Option 2: Virtualenv and Virtualenrwrapper</a>\n",
    "- <a href='#4'>Optional libraries</a>\n",
    "- <a href='#5'>Using the server</a>  \n",
    "    - <a href='#51'>Tunneling and ssh</a>  \n",
    "    - <a href='#52'>Using slurm</a>    \n",
    "- <a href='#6'>Using Keras with GPUs </a>  \n",
    "    - <a href='#61'>GPU usage monitoring </a>   \n",
    "    - <a href='#62'>Avoid using GPUs</a>   \n",
    "- <a href='#7'>Using TensorFlow on demand</a>  \n",
    "    - <a href='#71'>If you use *TF* with *Keras*</a>   \n",
    "    - <a href='#72'>If you use *Keras* that brings *TF*</a>  \n",
    "    - <a href='#73'>If *TF* is used alone without *Keras*</a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"1\">Connecting to the GPU server</a> \n",
    "\n",
    "First of all we must access via ssh:\n",
    "\n",
    "```\n",
    "$ ssh <user>@<remote_server>\n",
    "```\n",
    "\n",
    "Then you must insert your password and that's all, you're in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"2\">Changing your default password</a>  \n",
    "\n",
    "After you log in to your server with SSH:\n",
    "\n",
    "1. Enter the command:\n",
    "\n",
    "```\n",
    "$ passwd\n",
    "```\n",
    "\n",
    "2. Type your password, then press Enter.\n",
    "\n",
    "3. When prompted for your current UNIX password, enter your SSH password, then press Enter.\n",
    "\n",
    "4. Retype your new password and press enter. If successful, you will see the output:\n",
    "\n",
    "```\n",
    "$ passwd: all authentication tokes updated successfully\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"3\">Environment configuration</a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "83081d48-bd46-4014-ad86-e76179df864a"
    }
   },
   "source": [
    "## <a id=\"31\"> Option 1: Conda</a>  \n",
    "\n",
    "There are two major package managers in Python: `pip` and `conda`. `conda`, besides being a package manager is also useful as a version manager. There are two main ways to install conda: [Anaconda](https://conda.io/docs/install/quick.html) and [Miniconda](https://conda.io/miniconda.html).\n",
    "\n",
    "In order to install tensorflow we recommend following the [official documentation](https://www.tensorflow.org/install/install_linux#installing_with_anaconda). In particular, for the `conda` installation, they advise to use `pip` instead of `conda` as the only available Anaconda package for `tensorflow` is not actively mantained.\n",
    "\n",
    "All the available tensorflow versions (for both Python 2 and 3 and with CPU and GPU support) can be found [in this link](https://www.tensorflow.org/install/install_linux#top_of_page). There're CPU (https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp35-cp35m-linux_x86_64.whl) and GPU (https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp35-cp35m-linux_x86_64.whl)versions of tensorflow. But as we are using a GPU server, we recommended to use the GPU version (otherwise, what's the joke?).\n",
    "\n",
    "\n",
    "The commands to setup the environment are the following:\n",
    "\n",
    "```\n",
    "$ wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "$ bash Miniconda3-latest-Linux-x86_64.sh\n",
    "$ conda create --name <your_env> python=3.5\n",
    "$ source activate <your_env>\n",
    "(<your_env>) $ conda install numpy scipy scikit-learn jupyter nb_conda keras\n",
    "(<your_env>) $ pip install --ignore-installed --upgrade YOUR_TENSORFLOW_URL\n",
    "(<your_env>) $ jupyter notebook\n",
    "```\n",
    "\n",
    "__Note:__ It's quite important to install `keras` before `tensorflow`, as it overwrites the `tensorflow` version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"32\">Option 2: Virtualenv and Virtualenrwrapper</a>  \n",
    "\n",
    "```\n",
    "$ pip install --user virtualenv virtualenvwrapper\n",
    "```\n",
    "\n",
    "Then add the following lines to the bottom of you `~/.bashrc` file:\n",
    "\n",
    "```\n",
    "# virtualenv and virtualenvwrapper settings\n",
    "export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3\n",
    "[[ -s \"$HOME/.local/bin/virtualenvwrapper.sh\" ]] && source \"$HOME/.local/bin/virtualenvwrapper.sh\"\n",
    "```\n",
    "\n",
    "Update changes:\n",
    "\n",
    "```\n",
    "$ source ~/.bashrc\n",
    "```\n",
    "\n",
    "You should see an output like this:\n",
    "\n",
    "```\n",
    "virtualenvwrapper.user_scripts creating /home/<username>/.virtualenvs/premkproject\n",
    "virtualenvwrapper.user_scripts creating /home/<username>/.virtualenvs/postmkproject\n",
    "virtualenvwrapper.user_scripts creating /home/<username>/.virtualenvs/initialize\n",
    "virtualenvwrapper.user_scripts creating /home/<username>/.virtualenvs/premkvirtualenv\n",
    "virtualenvwrapper.user_scripts creating /home/<username>/.virtualenvs/postmkvirtualenv\n",
    "virtualenvwrapper.user_scripts creating /home/<username>/.virtualenvs/prermvirtualenv\n",
    "virtualenvwrapper.user_scripts creating /home/<username>/.virtualenvs/postrmvirtualenv\n",
    "virtualenvwrapper.user_scripts creating /home/<username>/.virtualenvs/predeactivate\n",
    "virtualenvwrapper.user_scripts creating /home/<username>/.virtualenvs/postdeactivate\n",
    "virtualenvwrapper.user_scripts creating /home/<username>/.virtualenvs/preactivate\n",
    "virtualenvwrapper.user_scripts creating /home/<username>/.virtualenvs/postactivate\n",
    "virtualenvwrapper.user_scripts creating /home/<username>/.virtualenvs/get_env_details\n",
    "```\n",
    "\n",
    "Create virtualenv:\n",
    "\n",
    "```\n",
    "$ mkvirtualenv <name> --python=/usr/bin/python3\n",
    "```\n",
    "\n",
    "Activate virtualenv:\n",
    "\n",
    "```\n",
    "$ source /users/<your_user>/.virtualenvs/<your_env>/bin/activate\n",
    "```\n",
    "\n",
    "Deactivate virtualenv:\n",
    "\n",
    "```\n",
    "$ deactivate\n",
    "```\n",
    "\n",
    "Additional Python libs:\n",
    "\n",
    "Install other Python libraries with: `$ pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "63d0205d-fcb9-4960-9aa4-5dde42fa0d9d"
    }
   },
   "source": [
    "# <a id=\"4\">Optional libraries</a>  \n",
    "\n",
    "These are some optional libraries to download in order to see some visualizations. They take a while, so if you don't have good Internet connection or no time you can skip them.\n",
    "\n",
    "To visualize keras graphs:\n",
    "```\n",
    "(<your_env>) $ pip install pydot pydotplus\n",
    "(<your_env>) $ conda install graphviz matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "40ceb1c0-ec6e-42c1-ac33-b280654bbeff"
    }
   },
   "source": [
    "# <a id=\"5\">Using the server</a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "403db3e2-61fb-44ec-8a1f-66e6a8cf2724"
    }
   },
   "source": [
    "## <a id=\"51\">Tunneling and ssh</a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "83314125-05e5-4287-81e5-9743b5377b37"
    }
   },
   "source": [
    "**How do you run a notebook in a remote machine?** You use an ssh connection with a port forwarding. This way, everything that goes to the port on the server machine (like a jupyter notebook) also goes to your localhost.\n",
    "\n",
    "It is likely that everyone will be using the same ports, so we recommend you to select a random number before connecting. The port on the ssh must be the same that you use to start the notebook.\n",
    "\n",
    "```\n",
    "$ ssh -L PORT:localhost:PORT USER@SERVER\n",
    "$ source activate <your_env>\n",
    "(<your_env>) $ jupyter notebook --port PORT --no-browser\n",
    "```\n",
    "\n",
    "Now you can use the notebook as if it were running on your computer, simply go to: \n",
    "\n",
    "`http://localhost:/?token=<token>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "46315e56-760b-49d9-b80a-38c8128a48b8"
    }
   },
   "source": [
    "## <a id=\"52\">Using slurm</a>  \n",
    "\n",
    "If the server uses a queue system called `slurm`, which grants exclusive access to the CPU resources. You should enqueue everythin you do that takes more than 10 minutes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ea93ab85-f784-4227-8ffb-08a30785809e"
    }
   },
   "source": [
    "#### Set up\n",
    "\n",
    "1. Download the script https://raw.githubusercontent.com/MIREL-UNC/mirel-scripts/master/run_scripts/submit_job_slurm.sh\n",
    "\n",
    "2. Create a logs folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f02f2744-895c-4f50-9ec9-dd77032320a2"
    }
   },
   "source": [
    "#### Enqueue things\n",
    "\n",
    "To enqueue a job on slurm, first put your command in a file, for example command.txt\n",
    "`\n",
    "$ sbatch submit_job_slurm.sh commant.txt\n",
    "`\n",
    "\n",
    "The queue will assign your job a number `JOBID`. All the output of your process will be redirected to `logs/JOBID.out` and `logs/JOBID.err`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e259cd21-1e9d-45d0-b083-bfff41bdeacc"
    }
   },
   "source": [
    "#### Controlling things \n",
    "\n",
    "To see the state of the queue run `$ squeue`\n",
    "\n",
    "To cancel a job run `$ scancel JOBID`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "461d4e4b-1de1-4325-88ea-c43fb21a36dc"
    }
   },
   "source": [
    "# <a id=\"6\">Using Keras with GPUs </a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "75917b10-4d4e-4270-92d7-b985c88d707e"
    }
   },
   "source": [
    "If you installed tensorflow with a GPU support, now it's a good time to check if it actually detects your devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "cb29ed84-fc63-452d-b3f8-062100aafa89"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "4f6101e6-5315-44f8-bf30-72e0a60bc9e1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7db34404-f64b-49dd-9fdd-f578ddbdd823"
    }
   },
   "source": [
    "If the above gives an error, try setting the environment variables. You can add this to your `.bashrc`, the changes are only temporary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "c61e9b3d-07c0-494c-b38f-8cf6040a19e3"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/cuda/9.0/extras/CUPTI/lib64/:/opt/cuda/9.0/lib64:/opt/cudnn/v7.0/\n",
    "export CUDA_HOME=/opt/cuda/9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"61\">GPU usage monitoring</a>  \n",
    "\n",
    "For Nvidia GPUs there is a tool `nvidia-smi` that can show memory usage, GPU utilization and temperature of GPU. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Watch command:\n",
    "\n",
    "`watch` is used to run any designated command at regular intervals. The time interval between reports can be changed from its default two seconds by using the `-n` option followed by an integer which represents the desired number of seconds. So, we can use the `watch` command together with `nvidia-smi`, in the following way:\n",
    "\n",
    "```\n",
    "$ watch -n 1 nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e1a8edab-adfc-45dc-8d16-efbe7d6ce69a"
    }
   },
   "source": [
    "## <a id=\"62\">Avoid using GPUs</a>  \n",
    "\n",
    "If all the GPUs are being used, you can still force Keras to use the CPU. For simple models this is still a very good option.\n",
    "\n",
    "The easiest way is to run you command with `CUDA_VISIBLE_DEVICES=\"\"`. For example:\n",
    "\n",
    "```\n",
    "(<your_env>) $ CUDA_VISIBLE_DEVICES=\"\" jupyter notebook --no-browser\n",
    "(<your_env>) $ CUDA_VISIBLE_DEVICES=\"\" exercise.py --experiment_name exp_000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all you have to do is have a little fun and take advantage of the benefits of GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"7\">Using TensorFlow on demand </a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"71\">If you use *TF* with *Keras*</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"72\">If you use *Keras* that brings *TF*</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "K.set_session(tf.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"73\">If *TF* is used alone without *Keras*</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "with tf.Session(config=config) as sess:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work is based on [Documentation](https://michaelblogscode.wordpress.com/2017/10/10/reducing-and-profiling-gpu-memory-usage-in-keras-with-tensorflow-backend/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
